diff --git a/include/llama.h b/include/llama.h
index 132937a0..5a7a7df2 100644
--- a/include/llama.h
+++ b/include/llama.h
@@ -1205,6 +1205,8 @@ extern "C" {
 
     LLAMA_API void llama_perf_dump_yaml(FILE * stream, const struct llama_context * ctx);
 
+    LLAMA_API bool llama_supports_embedding_only (const struct llama_context * ctx);
+
 #ifdef __cplusplus
 }
 #endif
diff --git a/src/llama-vocab.cpp b/src/llama-vocab.cpp
index a771eccd..57b50a3c 100644
--- a/src/llama-vocab.cpp
+++ b/src/llama-vocab.cpp
@@ -265,7 +265,13 @@ private:
             // output any symbols that did not form tokens as bytes.
             output.reserve(output.size() + symbol.n);
             for (int j = 0; j < (int)symbol.n; ++j) {
-                llama_vocab::id token_id = llama_byte_to_token_impl(vocab, symbol.text[j]);
+                llama_vocab::id token_id;
+                try {
+                    token_id = llama_byte_to_token_impl(vocab, symbol.text[j]);
+                } catch (const std::exception & e) {
+                    // not found, use PAD token instead.
+                    token_id = vocab.special_pad_id;
+                }
                 output.push_back(token_id);
             }
             return;
diff --git a/src/llama.cpp b/src/llama.cpp
index a718de05..06efb45b 100644
--- a/src/llama.cpp
+++ b/src/llama.cpp
@@ -6400,6 +6400,134 @@ static void llm_load_vocab(
     }
     GGML_ASSERT(vocab.id_to_token.size() == vocab.token_to_id.size());
 
+    // special tokens
+    {
+      const std::vector<std::pair<enum llm_kv, int32_t &>> special_token_types = {
+          { LLM_KV_TOKENIZER_BOS_ID,    vocab.special_bos_id    },
+          { LLM_KV_TOKENIZER_EOS_ID,    vocab.special_eos_id    },
+          { LLM_KV_TOKENIZER_UNK_ID,    vocab.special_unk_id    },
+          { LLM_KV_TOKENIZER_SEP_ID,    vocab.special_sep_id    },
+          { LLM_KV_TOKENIZER_PAD_ID,    vocab.special_pad_id    },
+          { LLM_KV_TOKENIZER_CLS_ID,    vocab.special_cls_id    },
+          { LLM_KV_TOKENIZER_MASK_ID,   vocab.special_mask_id   },
+          { LLM_KV_TOKENIZER_PREFIX_ID, vocab.special_prefix_id },
+          { LLM_KV_TOKENIZER_SUFFIX_ID, vocab.special_suffix_id },
+          { LLM_KV_TOKENIZER_MIDDLE_ID, vocab.special_middle_id },
+          { LLM_KV_TOKENIZER_EOT_ID,    vocab.special_eot_id    },
+          { LLM_KV_TOKENIZER_EOM_ID,    vocab.special_eom_id    },
+      };
+
+      for (const auto & it : special_token_types) {
+        const std::string & key = kv(std::get<0>(it));
+        int32_t & id = std::get<1>(it);
+
+        uint32_t new_id;
+        if (!ml.get_key(std::get<0>(it), new_id, false)) {
+          continue;
+        }
+        if (new_id >= vocab.id_to_token.size()) {
+          LLAMA_LOG_WARN("%s: bad special token: '%s' = %ud, using default id %d\n",
+                         __func__, key.c_str(), new_id, id);
+        } else {
+          id = new_id;
+        }
+      }
+
+      // Handle add_bos_token and add_eos_token
+      {
+        bool temp = true;
+
+        if (ml.get_key(LLM_KV_TOKENIZER_ADD_BOS, temp, false)) {
+          vocab.tokenizer_add_bos = temp;
+        }
+        if (ml.get_key(LLM_KV_TOKENIZER_ADD_EOS, temp, false)) {
+          vocab.tokenizer_add_eos = temp;
+        }
+      }
+
+      // find EOT token: "<|eot_id|>", "<|im_end|>", "<end_of_turn>", etc.
+      //
+      // TODO: convert scripts should provide this token through the KV metadata LLAMA_KV_TOKENIZER_EOT_ID
+      //       for now, we apply this workaround to find the EOT token based on its text
+      if (vocab.special_eot_id == -1) {
+        for (const auto & t : vocab.token_to_id) {
+          if (false
+              // TODO: gemma "<end_of_turn>" is exported as a normal token, so the following check does not work
+              //       need to fix convert script
+              //vocab.id_to_token[t.second].type == LLAMA_TOKEN_TYPE_CONTROL &&
+              || t.first == "<|eot_id|>"
+              || t.first == "<|im_end|>"
+              || t.first == "<|end|>"
+              || t.first == "<end_of_turn>"
+              || t.first == "<|endoftext|>"
+              || t.first == "<EOT>"
+          ) {
+            vocab.special_eot_id = t.second;
+            if ((vocab.id_to_token[t.second].attr & LLAMA_TOKEN_ATTR_CONTROL) == 0) {
+              LLAMA_LOG_WARN("%s: control-looking token: '%s' was not control-type; this is probably a bug in the model. its type will be overridden\n",
+                             __func__, t.first.c_str());
+              vocab.id_to_token[t.second].attr = LLAMA_TOKEN_ATTR_CONTROL;
+            }
+            break;
+          }
+        }
+      }
+
+      // find EOM token: "<|eom_id|>"
+      //
+      // TODO: convert scripts should provide this token through the KV metadata LLAMA_KV_TOKENIZER_EOM_ID
+      //       for now, we apply this workaround to find the EOM token based on its text
+      if (vocab.special_eom_id == -1) {
+        const auto & t = vocab.token_to_id.find("<|eom_id|>");
+        if (t != vocab.token_to_id.end()) {
+          vocab.special_eom_id = t->second;
+          if ((vocab.id_to_token[t->second].attr & LLAMA_TOKEN_ATTR_CONTROL) == 0) {
+            LLAMA_LOG_WARN("%s: control-looking token: '%s' was not control-type; this is probably a bug in the model. its type will be overridden\n",
+                           __func__, t->first.c_str());
+            vocab.id_to_token[t->second].attr = LLAMA_TOKEN_ATTR_CONTROL;
+          }
+        }
+      }
+
+      // maintain a list of tokens that cause end-of-generation
+      // this is currently determined based on the token text, which is obviously not ideal
+      // ref: https://github.com/ggerganov/llama.cpp/issues/9606
+      vocab.special_eog_ids.clear();
+      for (const auto & t : vocab.token_to_id) {
+        if (false
+            || t.first == "<|eot_id|>"
+            || t.first == "<|im_end|>"
+            || t.first == "<|end|>"
+            || t.first == "<end_of_turn>"
+            || t.first == "<|endoftext|>"
+            || t.first == "<|eom_id|>"
+            || t.first == "<EOT>"
+        ) {
+          vocab.special_eog_ids.insert(t.second);
+          if ((vocab.id_to_token[t.second].attr & LLAMA_TOKEN_ATTR_CONTROL) == 0) {
+            LLAMA_LOG_WARN("%s: control-looking token: '%s' was not control-type; this is probably a bug in the model. its type will be overridden\n",
+                           __func__, t.first.c_str());
+            vocab.id_to_token[t.second].attr = LLAMA_TOKEN_ATTR_CONTROL;
+          }
+        }
+      }
+
+      if (vocab.special_eos_id != -1 && vocab.special_eog_ids.count(vocab.special_eos_id) == 0) {
+        vocab.special_eog_ids.insert(vocab.special_eos_id);
+        LLAMA_LOG_WARN("%s: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n", __func__);
+      }
+
+      if (vocab.special_eot_id != -1 && vocab.special_eog_ids.count(vocab.special_eot_id) == 0) {
+        vocab.special_eog_ids.insert(vocab.special_eot_id);
+        LLAMA_LOG_WARN("%s: special_eot_id is not in special_eog_ids - the tokenizer config may be incorrect\n", __func__);
+      }
+
+      if (vocab.special_eom_id != -1 && vocab.special_eog_ids.count(vocab.special_eom_id) == 0) {
+        vocab.special_eog_ids.insert(vocab.special_eom_id);
+        LLAMA_LOG_WARN("%s: special_eom_id is not in special_eog_ids - the tokenizer config may be incorrect\n", __func__);
+      }
+    }
+
     // determine the newline token: LLaMA "<0x0A>" == 10 == '\n', Falcon 193 == '\n'
     if (vocab.type == LLAMA_VOCAB_TYPE_SPM) {
         // For Fill-In-the-Middle (FIM)/infill models which where converted
@@ -6458,134 +6586,6 @@ static void llm_load_vocab(
         vocab.linefeed_id = ids[0];
     }
 
-    // special tokens
-    {
-        const std::vector<std::pair<enum llm_kv, int32_t &>> special_token_types = {
-            { LLM_KV_TOKENIZER_BOS_ID,    vocab.special_bos_id    },
-            { LLM_KV_TOKENIZER_EOS_ID,    vocab.special_eos_id    },
-            { LLM_KV_TOKENIZER_UNK_ID,    vocab.special_unk_id    },
-            { LLM_KV_TOKENIZER_SEP_ID,    vocab.special_sep_id    },
-            { LLM_KV_TOKENIZER_PAD_ID,    vocab.special_pad_id    },
-            { LLM_KV_TOKENIZER_CLS_ID,    vocab.special_cls_id    },
-            { LLM_KV_TOKENIZER_MASK_ID,   vocab.special_mask_id   },
-            { LLM_KV_TOKENIZER_PREFIX_ID, vocab.special_prefix_id },
-            { LLM_KV_TOKENIZER_SUFFIX_ID, vocab.special_suffix_id },
-            { LLM_KV_TOKENIZER_MIDDLE_ID, vocab.special_middle_id },
-            { LLM_KV_TOKENIZER_EOT_ID,    vocab.special_eot_id    },
-            { LLM_KV_TOKENIZER_EOM_ID,    vocab.special_eom_id    },
-        };
-
-        for (const auto & it : special_token_types) {
-            const std::string & key = kv(std::get<0>(it));
-            int32_t & id = std::get<1>(it);
-
-            uint32_t new_id;
-            if (!ml.get_key(std::get<0>(it), new_id, false)) {
-                continue;
-            }
-            if (new_id >= vocab.id_to_token.size()) {
-                LLAMA_LOG_WARN("%s: bad special token: '%s' = %ud, using default id %d\n",
-                    __func__, key.c_str(), new_id, id);
-            } else {
-                id = new_id;
-            }
-        }
-
-        // Handle add_bos_token and add_eos_token
-        {
-            bool temp = true;
-
-            if (ml.get_key(LLM_KV_TOKENIZER_ADD_BOS, temp, false)) {
-                vocab.tokenizer_add_bos = temp;
-            }
-            if (ml.get_key(LLM_KV_TOKENIZER_ADD_EOS, temp, false)) {
-                vocab.tokenizer_add_eos = temp;
-            }
-        }
-
-        // find EOT token: "<|eot_id|>", "<|im_end|>", "<end_of_turn>", etc.
-        //
-        // TODO: convert scripts should provide this token through the KV metadata LLAMA_KV_TOKENIZER_EOT_ID
-        //       for now, we apply this workaround to find the EOT token based on its text
-        if (vocab.special_eot_id == -1) {
-            for (const auto & t : vocab.token_to_id) {
-                if (false
-                        // TODO: gemma "<end_of_turn>" is exported as a normal token, so the following check does not work
-                        //       need to fix convert script
-                        //vocab.id_to_token[t.second].type == LLAMA_TOKEN_TYPE_CONTROL &&
-                        || t.first == "<|eot_id|>"
-                        || t.first == "<|im_end|>"
-                        || t.first == "<|end|>"
-                        || t.first == "<end_of_turn>"
-                        || t.first == "<|endoftext|>"
-                        || t.first == "<EOT>"
-                   ) {
-                    vocab.special_eot_id = t.second;
-                    if ((vocab.id_to_token[t.second].attr & LLAMA_TOKEN_ATTR_CONTROL) == 0) {
-                        LLAMA_LOG_WARN("%s: control-looking token: '%s' was not control-type; this is probably a bug in the model. its type will be overridden\n",
-                                __func__, t.first.c_str());
-                        vocab.id_to_token[t.second].attr = LLAMA_TOKEN_ATTR_CONTROL;
-                    }
-                    break;
-                }
-            }
-        }
-
-        // find EOM token: "<|eom_id|>"
-        //
-        // TODO: convert scripts should provide this token through the KV metadata LLAMA_KV_TOKENIZER_EOM_ID
-        //       for now, we apply this workaround to find the EOM token based on its text
-        if (vocab.special_eom_id == -1) {
-            const auto & t = vocab.token_to_id.find("<|eom_id|>");
-            if (t != vocab.token_to_id.end()) {
-                vocab.special_eom_id = t->second;
-                if ((vocab.id_to_token[t->second].attr & LLAMA_TOKEN_ATTR_CONTROL) == 0) {
-                    LLAMA_LOG_WARN("%s: control-looking token: '%s' was not control-type; this is probably a bug in the model. its type will be overridden\n",
-                        __func__, t->first.c_str());
-                    vocab.id_to_token[t->second].attr = LLAMA_TOKEN_ATTR_CONTROL;
-                }
-            }
-        }
-
-        // maintain a list of tokens that cause end-of-generation
-        // this is currently determined based on the token text, which is obviously not ideal
-        // ref: https://github.com/ggerganov/llama.cpp/issues/9606
-        vocab.special_eog_ids.clear();
-        for (const auto & t : vocab.token_to_id) {
-            if (false
-                    || t.first == "<|eot_id|>"
-                    || t.first == "<|im_end|>"
-                    || t.first == "<|end|>"
-                    || t.first == "<end_of_turn>"
-                    || t.first == "<|endoftext|>"
-                    || t.first == "<|eom_id|>"
-                    || t.first == "<EOT>"
-               ) {
-                vocab.special_eog_ids.insert(t.second);
-                if ((vocab.id_to_token[t.second].attr & LLAMA_TOKEN_ATTR_CONTROL) == 0) {
-                    LLAMA_LOG_WARN("%s: control-looking token: '%s' was not control-type; this is probably a bug in the model. its type will be overridden\n",
-                            __func__, t.first.c_str());
-                    vocab.id_to_token[t.second].attr = LLAMA_TOKEN_ATTR_CONTROL;
-                }
-            }
-        }
-
-        if (vocab.special_eos_id != -1 && vocab.special_eog_ids.count(vocab.special_eos_id) == 0) {
-            vocab.special_eog_ids.insert(vocab.special_eos_id);
-            LLAMA_LOG_WARN("%s: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n", __func__);
-        }
-
-        if (vocab.special_eot_id != -1 && vocab.special_eog_ids.count(vocab.special_eot_id) == 0) {
-            vocab.special_eog_ids.insert(vocab.special_eot_id);
-            LLAMA_LOG_WARN("%s: special_eot_id is not in special_eog_ids - the tokenizer config may be incorrect\n", __func__);
-        }
-
-        if (vocab.special_eom_id != -1 && vocab.special_eog_ids.count(vocab.special_eom_id) == 0) {
-            vocab.special_eog_ids.insert(vocab.special_eom_id);
-            LLAMA_LOG_WARN("%s: special_eom_id is not in special_eog_ids - the tokenizer config may be incorrect\n", __func__);
-        }
-    }
-
     // build special tokens cache
     {
         for (llama_vocab::id id = 0; id < (llama_vocab::id)n_vocab; ++id) {
@@ -18823,7 +18823,7 @@ struct llama_context * llama_new_context_with_model(
         cparams.n_batch = GGML_KQ_MASK_PAD;
     }
 
-    cparams.n_ubatch         = std::min(cparams.n_batch, params.n_ubatch == 0 ? params.n_batch : params.n_ubatch);
+    cparams.n_ubatch         = hparams.causal_attn ? std::min(cparams.n_batch, params.n_ubatch == 0 ? params.n_batch : params.n_ubatch) : cparams.n_batch;
 
     cparams.n_ctx_orig_yarn  = params.yarn_orig_ctx    != 0 ? params.yarn_orig_ctx    :
                                hparams.n_ctx_orig_yarn != 0 ? hparams.n_ctx_orig_yarn :
@@ -21450,3 +21450,7 @@ void llama_log_callback_default(ggml_log_level level, const char * text, void *
     fputs(text, stderr);
     fflush(stderr);
 }
+
+bool llama_supports_embedding_only(const struct llama_context * ctx) {
+    return !ctx->cparams.causal_attn;
+}
\ No newline at end of file
